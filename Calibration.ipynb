{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f29a437",
   "metadata": {},
   "source": [
    "## TOOL UPDATED: December 3 2024, Henrik Loecke\n",
    "\n",
    "##Double click this cell to see full description.\n",
    "\n",
    "##You must restart the kernel after updating Calibration_Variables.py!\n",
    "\n",
    "<!-- \n",
    "\n",
    "To run this notebook, click menu Cell -> Run All\n",
    "\n",
    "User input has been moved away from this notebook so it can easily be replaced by new versions.\n",
    " \n",
    "Please open Calibration_Variables.py, in the same folder as this notebook, to edit user input there.\n",
    "\n",
    "All variables with path must start with 'r', e.g. r'C:\\Projects'\n",
    "\n",
    "It must contain the following variables:\n",
    "\n",
    "model_area:                          Short area name like 'VSA' or LISA'\n",
    "generate_confidence_csvs:            Generate csv files for confidence maps, True/False\n",
    "result_specs_csv:                    CSV file linking network and runoff result file. Only needed if runoff imported.\n",
    "map_point_spacing:                   Space between dots in confidence maps, e.g. 100 (in meter but number withouth unit)\n",
    "use_accumulation:                    Use for models with proper tree structure, all models except VSA, True/False\n",
    "slope_source_unit_meter_per_meter    This is the case for NSSA and FSA, in VSA it is per thousand\n",
    "model_area_strict_match:             If True, accept 'VSA' but not 'VSA-2019'. If False, accept both.\n",
    "output_folder:                       Folder path where reports are to be created.\n",
    "result_folder:                       Folder path of result files.\n",
    "calibration_sheet:                   Full path, folder included, of calibration parameter sheet.\n",
    "model:                               Full path, folder included, of model database.\n",
    "summation_csv:                       Full path, folder included, of summation.csv.\n",
    "node_csv:                            Full path, folder included, of MH_Zones.csv.\n",
    "outfall_csv:                         Full path, folder included, of Outfall_Summary.csv.\n",
    "rainfall_dfs0_file:                  Full path, folder included, of rainfall dfs0 file.\n",
    "map_folder:                          Folder path where report maps are located.\n",
    "dfs0_folders:                        Python list of all folders holding measurement dfs0 files.\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0febcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMANENT CELL 1\n",
    "\n",
    "import os\n",
    "import re\n",
    "import mikeio\n",
    "import mikeio1d\n",
    "from mikeio1d.res1d import Res1D\n",
    "from mikeio.dfs0 import Dfs0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import pickle\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "import sqlite3\n",
    "from Calibration_Variables import *\n",
    "import copy\n",
    "import ctypes\n",
    "import traceback\n",
    "import shutil\n",
    "import subprocess\n",
    "MessageBox = ctypes.windll.user32.MessageBoxA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3f2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMANENT CELL 2\n",
    "#The external script Read_Parameters.py is called to read data from the model.\n",
    "try:\n",
    "    \n",
    "    \n",
    "    if not (model.lower().endswith('.sqlite') or model.lower().endswith('.mdb')):\n",
    "        raise ValueError(\"The variable 'model' must have .mdb or .sqlite extension: \" + model)\n",
    "    if os.path.exists(model) == False:\n",
    "        raise ValueError(\"The variable 'model' points to a path that does not exist: \" + model)    \n",
    "    \n",
    "    parameter_script = r\"Read_Parameters.py\"\n",
    "    bat_file_path = 'Read_Parameters.bat'\n",
    "    bat_file = open(bat_file_path, \"w\")\n",
    "    bat_file.write(python_installation + ' \"' + parameter_script + '\" \"' + os.getcwd() + '\" \"' + model + '\" ' + str(use_accumulation))\n",
    "    bat_file.close()\n",
    "    result = subprocess.call([bat_file_path]) \n",
    "\n",
    "    if result == 1: #Error\n",
    "        raise ValueError(\"The sub process threw an error. Please Locate the bat file: \" + bat_file_path + \", open it in notepad, \\\n",
    "        then add a new line and type in letters only: Pause. Double click the bat file to run it and it will show the error.\")\n",
    "           \n",
    "except Exception as e:    \n",
    "    error_message = str(e)\n",
    "    MessageBox(None, b'An error happened in permanent cell 2\\n\\n' + error_message.encode('utf-8'), b\"Error\", 0)\n",
    "    raise ValueError(\"Error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdcac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMANENT CELL 3\n",
    "#Import csv files and set up different lists used later in the notebook.\n",
    "\n",
    "try:\n",
    "    dwf_csv = r\"DWF_Specs.csv\"\n",
    "    res_types = ['Mixed','ResHD','ResLD']\n",
    "    ici_types = ['Commercial','Industrial','Institutional']\n",
    "    ww_types = res_types + ici_types\n",
    "\n",
    "    all_types = []\n",
    "    for res_type in res_types:\n",
    "        all_types.append([res_type,'Population','pe','m3/pe/d'])\n",
    "    for ici_type in ici_types:\n",
    "        all_types.append([ici_type,'Area','ha','m3/ha/d'])\n",
    "    all_types.append(['Baseflow','','',''])\n",
    "    all_types.append(['Total','','',''])\n",
    "    all_types = pd.DataFrame(all_types, columns =['Load_Type','Description','Unit1','Unit2'])\n",
    "    all_types.set_index('Load_Type',inplace=True)\n",
    "\n",
    "    period_specs = []\n",
    "    period_specs.append('DWF')\n",
    "    for i in range(1,7):\n",
    "        period_specs.append('WWF' + str(i))\n",
    "    period_specs\n",
    "\n",
    "    dwf_specs = pd.read_csv(os.getcwd() + '\\\\' + dwf_csv)\n",
    "    dwf_specs.set_index('Zone',inplace=True)\n",
    "    \n",
    "    network_specs = pd.read_csv(os.getcwd() + '\\\\Network.csv')\n",
    "    network_specs.set_index('MUID',inplace=True)\n",
    "\n",
    "    wwf_csv = r\"WWF_Specs.csv\"\n",
    "\n",
    "    #used to maintain consistent color order for overflows\n",
    "    colors = ['black','purple','green','orange','grey','brown']\n",
    "\n",
    "    gauges = pd.read_excel(calibration_sheet,sheet_name=\"Gauges\")\n",
    "    gauges = gauges.loc[:, :'Shift Y (m)']\n",
    "    if model_area_strict_match == True:\n",
    "        gauges = gauges[gauges.Model==model_area]\n",
    "    else:\n",
    "        gauges = gauges[gauges.Model.str.contains(model_area)]\n",
    "    gauges.set_index('Gauge',inplace=True)\n",
    "\n",
    "\n",
    "    periods = pd.read_excel(calibration_sheet,sheet_name=\"Periods\")\n",
    "    if model_area_strict_match == True:\n",
    "        periods = periods[periods.Model==model_area]\n",
    "    else:\n",
    "        periods = periods[periods.Model.str.contains(model_area)]\n",
    "\n",
    "    report_text = pd.read_excel(calibration_sheet,sheet_name=\"Report_Text\")\n",
    "    report_text = report_text[report_text.Model.notna()]\n",
    "    if model_area_strict_match == True:\n",
    "        report_text = report_text[report_text.Model==model_area]\n",
    "    else:\n",
    "        report_text = report_text[report_text.Model.str.contains(model_area)]\n",
    "    report_text.fillna('This section was left blank.', inplace=True)\n",
    "\n",
    "    wwf_specs = pd.read_csv(wwf_csv)\n",
    "    wwf_specs.set_index('Location',inplace=True)\n",
    "    wwf_stats_specs = pd.read_csv('WWF_Stats_Specs.csv')\n",
    "\n",
    "    diurnals = pd.read_csv('Diurnals.csv')\n",
    "\n",
    "    if generate_confidence_csvs:\n",
    "\n",
    "        map_periods = pd.read_excel(calibration_sheet,sheet_name=\"Periods_Map\")\n",
    "        map_periods = map_periods.loc[:, :'Model']\n",
    "        if model_area_strict_match == True:\n",
    "            map_periods = map_periods[map_periods.Model==model_area]\n",
    "        else:\n",
    "            map_periods = map_periods[map_periods.Model.str.contains(model_area)]\n",
    "\n",
    "        thresholds = pd.read_excel(calibration_sheet,sheet_name=\"Thresholds\",skiprows=1)\n",
    "        thresholds.rename(columns={'Unnamed: 0':'Color'},inplace=True)\n",
    "        for column in thresholds.columns[1:]:\n",
    "            thresholds.rename(columns={column:column + ' ' + thresholds.loc[0,column]},inplace=True)\n",
    "        for column in thresholds.columns[1:]:\n",
    "            thresholds.rename(columns={column:column.replace('.1','')},inplace=True)\n",
    "            thresholds.rename(columns={column:column.replace('.2','')},inplace=True)\n",
    "        thresholds.drop(index=0,inplace=True)\n",
    "        thresholds.drop(thresholds.index[3:],inplace=True) \n",
    "        thresholds.sort_values(by='DWF Flow Status',ascending=False,inplace=True)\n",
    "        thresholds.reset_index(inplace=True)\n",
    "\n",
    "        map_periods_check = map_periods.loc[:, :'Meter Status']\n",
    "        map_periods_check_count = map_periods_check[map_periods_check['Meter Status']=='Primary']\\\n",
    "            [['Zone','Meter Status']].groupby(['Zone']).count()\n",
    "        map_periods_check_count.rename(columns={'Meter Status':'Primary Count'},inplace=True)\n",
    "        map_periods_check = pd.merge(map_periods_check,map_periods_check_count,on=['Zone'],how='left')\n",
    "        multiple_primary = list(map_periods_check_count[map_periods_check_count['Primary Count']>1].index)\n",
    "\n",
    "        missing_primary = []\n",
    "        for zone in list(gauges.Location.unique()):\n",
    "            if not zone in (map_periods_check_count.index):\n",
    "                missing_primary.append(zone)\n",
    "\n",
    "        if len(multiple_primary) > 0 or len(missing_primary) > 0 :\n",
    "            error_message = 'Please correct the following errors in ' + os.path.basename(calibration_sheet) + ', sheet Periods_Map.\\n\\n'\n",
    "            error_message += 'The following zones have multiple primary meters: '  + ','.join(multiple_primary) + '.\\n'\n",
    "            error_message += 'The following zones have no primary meters: '  + ','.join(missing_primary) + '.\\n'\n",
    "            raise ValueError(error_message)\n",
    "\n",
    "except Exception as e: \n",
    "    error_message = str(e)\n",
    "    MessageBox(None, b'An error happened in permanent cell 3\\n\\n' + error_message.encode('utf-8'), b\"Error\", 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0243d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMANENT CELL 4\n",
    "#List all network elements to be imported.\n",
    "\n",
    "try:\n",
    "    nodes1 = gauges[gauges['Level Type']=='Node']['Node1 (Or Pipe if pipe level)'].dropna().unique().flatten().tolist()\n",
    "    nodes2 = gauges[gauges['Level Type']=='Node']['Node2'].dropna().unique().flatten().tolist()\n",
    "    level_nodes = nodes1 + nodes2\n",
    "    #Convert all to string\n",
    "    level_nodes = [str(x) for x in level_nodes]\n",
    "    #Make set to clear duplicates\n",
    "    level_nodes = set(level_nodes)\n",
    "\n",
    "    ds_level_pipes = gauges[gauges['Level Type']=='Link(DS)']['Node1 (Or Pipe if pipe level)'].dropna().unique().flatten().tolist()\n",
    "    us_level_pipes = gauges[gauges['Level Type']=='Link(US)']['Node1 (Or Pipe if pipe level)'].dropna().unique().flatten().tolist()\n",
    "    flow_pipes = gauges.Pipe.dropna().unique().flatten().tolist()\n",
    "\n",
    "    #Convert all to string\n",
    "    ds_level_pipes = [str(x) for x in ds_level_pipes]\n",
    "    us_level_pipes = [str(x) for x in us_level_pipes]\n",
    "    flow_pipes = [str(x) for x in flow_pipes]\n",
    "\n",
    "    #Add outfalls\n",
    "    outfalls = pd.read_csv(outfall_csv,dtype={'Weir': str,'Outfall': str})\n",
    "    outfalls['Res_ID'] = ''\n",
    "    for index, row in outfalls.iterrows():\n",
    "        prefix = ''\n",
    "        if row['Layer'].lower() != 'msm_link' and row['Layer'].lower() != 'summation':\n",
    "            prefix = row['Layer'][4:] + ':'\n",
    "        muid = prefix + row['Weir']\n",
    "\n",
    "        if row['Layer'].lower() != 'summation':\n",
    "            outfalls.iloc[index,3] = row['Layer'][4:]\n",
    "        outfalls.iloc[index,4] = muid\n",
    "\n",
    "        if row['Layer'].lower() != 'summation':\n",
    "            flow_pipes.append(muid)\n",
    "\n",
    "    catchments = set()\n",
    "    summation_df = pd.read_csv(summation_csv,dtype={'MUID': str,'SUMTO': str})        \n",
    "    for index, row in summation_df.iterrows():\n",
    "        prefix = ''\n",
    "\n",
    "        if row['Layer'].lower() != 'msm_link' and row['Layer'].lower() != 'ms_catchment' and row['Layer'].lower() != 'summation':\n",
    "            prefix = row['Layer'][row['Layer'].find('_') + 1:] + ':'\n",
    "\n",
    "        muid = prefix + row['MUID']\n",
    "\n",
    "        summation_df.iloc[index,0] = muid\n",
    "\n",
    "        if '-Negative' in muid:\n",
    "            muid = muid[:-9]\n",
    "\n",
    "        if row['Layer'].lower() == 'ms_catchment':\n",
    "            catchments.add(muid)\n",
    "        else:\n",
    "            flow_pipes.append(muid)\n",
    "\n",
    "    #Make set to clear duplicates\n",
    "    ds_level_pipes = set(ds_level_pipes)\n",
    "    us_level_pipes = set(us_level_pipes)\n",
    "    flow_pipes = set(flow_pipes)\n",
    "\n",
    "except Exception as e: \n",
    "    error_message = str(e)\n",
    "    MessageBox(None, b'An error happened in permanent cell 4\\n\\n' + error_message.encode('utf-8'), b\"Error\", 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6779bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMANENT CELL 5\n",
    "#Import dfs0 files\n",
    "\n",
    "try:\n",
    "\n",
    "    #Import rain gauges\n",
    "    rainfall = mikeio.read(rainfall_dfs0_file).to_dataframe()\n",
    "    warnings = []\n",
    "\n",
    "    #Import flow/level gauges\n",
    "    gauge_ids = []\n",
    "    first_dfs0 = True\n",
    "    for dfs0_folder in dfs0_folders:\n",
    "        for f in os.listdir(dfs0_folder):\n",
    "            if f[-5:]=='.dfs0':\n",
    "\n",
    "\n",
    "                if 'ps' in f.lower() and f[:2].lower() != 'ps':\n",
    "                    gauge_id = re.split(r'[.]',f)[0]\n",
    "                else:\n",
    "                    gauge_id = re.split(r'_|[.]',f)[0]\n",
    "                res = mikeio.read(dfs0_folder + '\\\\' + f)\n",
    "                ts = res.to_dataframe()\n",
    "\n",
    "                gauge_ids.append(gauge_id)\n",
    "\n",
    "                first_level = True\n",
    "                second_level = True\n",
    "                first_velocity = True\n",
    "                for i, column in enumerate(ts.columns):\n",
    "                    if i == 0:\n",
    "                        ts.rename(columns={ts.columns[0]:'Flow'},inplace=True)\n",
    "                        if 'meter pow 3 per sec' in str(res.items[0]):\n",
    "                            ts.Flow = ts.Flow * 1000\n",
    "                        elif 'liter per sec' in str(res.items[0]):\n",
    "                            ts.Flow = ts.Flow \n",
    "                        else:\n",
    "                            warnings.append('First item in ' + f + ' does not appear to be type Discharge. This is not imported.')\n",
    "                            ts.Flow = np.nan\n",
    "\n",
    "                    elif str(res.items[i])[-21:] == '<Water Level> (meter)': \n",
    "                        if first_level == True:\n",
    "                            ts.rename(columns={ts.columns[i]:'Level'},inplace=True)\n",
    "                            first_level = False\n",
    "                        elif second_level == True:\n",
    "                            ts.rename(columns={ts.columns[i]:'Level2'},inplace=True)\n",
    "                            second_level = False\n",
    "\n",
    "\n",
    "                    elif str(res.items[i])[-31:] == '<Flow velocity> (meter per sec)' and first_velocity == True:\n",
    "                        ts.rename(columns={ts.columns[i]:'Velocity'},inplace=True)\n",
    "                        first_velocity = False\n",
    "\n",
    "                if not 'Flow' in ts.columns:\n",
    "                    ts['Flow'] = np.nan                     \n",
    "                if not 'Velocity' in ts.columns:\n",
    "                    ts['Velocity'] = np.nan\n",
    "                if not 'Level' in ts.columns:\n",
    "                    ts['Level'] = np.nan\n",
    "                if not 'Level2' in ts.columns:\n",
    "                    ts['Level2'] = np.nan\n",
    "\n",
    "                ts['Gauge'] = gauge_id\n",
    "                ts = ts[['Gauge','Flow','Level','Level2','Velocity']]\n",
    "                ts['Seconds'] = ts.index.to_series().diff().astype('timedelta64[s]').fillna(method='bfill').astype('int64')\n",
    "                ts['Volume'] = ts.Flow * ts.Seconds / 1000\n",
    "                if first_dfs0 == True:\n",
    "                    measured = ts.copy()\n",
    "                else:\n",
    "                    measured = pd.concat([measured,ts])\n",
    "                first_dfs0 = False\n",
    "\n",
    "    for warning in warnings:\n",
    "        print (warning)\n",
    "\n",
    "except Exception as e: \n",
    "    error_message = str(e)\n",
    "    MessageBox(None, b'An error happened in permanent cell 5\\n\\n' + error_message.encode('utf-8'), b\"Error\", 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91611bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMANENT CELL 6\n",
    "#Import network (and runoff if applicable) and addout result files\n",
    "\n",
    "try:\n",
    "    node_zones = pd.read_csv(node_csv,dtype={'Node': str})\n",
    "    node_zones.set_index(node_zones.Node,inplace=True)\n",
    "    spill_zones = list(node_zones.Zone.unique())\n",
    "\n",
    "    first_round = True\n",
    "    for f in os.listdir(result_folder):\n",
    "        if '.ADDOUT.res1d' in f or ('.sqlite' in model and 'Network_HD.res1d' in f):\n",
    "\n",
    "            res1d = Res1D(result_folder + '\\\\' + f)\n",
    "            flood_types = ['WaterFlowRateAboveGround','WaterSpillDischarge']\n",
    "            cover_types = ['Normal','Spilling']\n",
    "\n",
    "            first_round_in_result = True\n",
    "            res_zones = set()\n",
    "            for node in res1d.data.Nodes:\n",
    "                muid = node.Id\n",
    "                for i, flood_type in enumerate(flood_types):\n",
    "                    ts = res1d.query.GetNodeValues(muid,flood_type)\n",
    "                    col_name = muid + '-' + cover_types[i]\n",
    "                    if ts != None:\n",
    "                        if max(ts) > 0 or first_round_in_result == True:\n",
    "                            spill_df = pd.DataFrame(index=res1d.time_index)\n",
    "                            spill_df['Node'] = muid\n",
    "                            zone = node_zones.loc[muid,'Zone']\n",
    "                            spill_df['Zone'] = zone\n",
    "                            spill_df['Spill'] = ts\n",
    "                            spill_df['Spill'] = spill_df['Spill'] * 1000\n",
    "\n",
    "                            res_zones.add(zone)\n",
    "\n",
    "                            if first_round == True:\n",
    "                                spill_df_all = spill_df.copy()\n",
    "                            else:\n",
    "                                spill_df_all = pd.concat([spill_df_all,spill_df])                                                        \n",
    "                            first_round = False\n",
    "                            first_round_in_result = False\n",
    "\n",
    "            #Create empty df for any zone not represented to show 0 on the graph.\n",
    "            for spill_zone in spill_zones:\n",
    "                if not spill_zone in res_zones:\n",
    "                    spill_df = pd.DataFrame(index=res1d.time_index)\n",
    "                    spill_df['Node'] = 'X'\n",
    "                    spill_df['Zone'] = spill_zone\n",
    "                    spill_df['Spill'] = 0           \n",
    "                    spill_df_all = pd.concat([spill_df_all,spill_df])                                                        \n",
    "\n",
    "    spill_df_zones = spill_df_all.copy()\n",
    "    spill_df_zones['Date_Time'] = spill_df_zones.index\n",
    "    spill_df_zones.drop(columns=['Node'],inplace=True)\n",
    "    spill_df_zones = spill_df_zones.groupby(['Zone','Date_Time']).sum()\n",
    "    spill_df_zones.reset_index(inplace=True)\n",
    "\n",
    "    first_runoff = True\n",
    "    if len(catchments) > 0:\n",
    "        for f in os.listdir(result_folder):\n",
    "            if 'RR.res1d' in f:\n",
    "\n",
    "    #             print('Opening ' + f)\n",
    "                res1d = Res1D(result_folder + '\\\\' + f)\n",
    "\n",
    "                for catchment in catchments:\n",
    "    #                 print('Importing ' + catchment)\n",
    "                    catchment_df = pd.DataFrame(index = res1d.time_index)\n",
    "                    catchment_df['ResultFile'] = f\n",
    "                    catchment_df['MUID'] = catchment\n",
    "                    catchment_df['DateTimeRef'] = catchment_df.index\n",
    "                    catchment_df['Discharge'] = res1d.query.GetCatchmentValues(catchment, \"TotalRunOff\")  \n",
    "                    catchment_df['Discharge'] = catchment_df['Discharge']*1000\n",
    "\n",
    "                    catchment_df['Seconds'] = catchment_df.index.to_series().diff().astype('timedelta64[s]').fillna(method='bfill').astype('int64')\n",
    "                    catchment_df['Volume'] = catchment_df.Discharge * catchment_df.Seconds / 1000\n",
    "                    catchment_df.drop(columns=['Seconds'],inplace=True)\n",
    "\n",
    "                    if first_runoff == True:\n",
    "                        catchment_df_all = catchment_df.copy()\n",
    "                    else:\n",
    "                        catchment_df_all = pd.concat([catchment_df_all,catchment_df])\n",
    "                    first_runoff = False\n",
    "\n",
    "        result_specs = pd.read_csv(result_specs_csv)\n",
    "        result_specs.rename(columns={'Runoff':'ResultFile'},inplace=True)\n",
    "        catchment_df_all = pd.merge(catchment_df_all,result_specs,how='left',on=['ResultFile'])\n",
    "        catchment_df_all.drop(columns='ResultFile',inplace=True)\n",
    "        catchment_df_all.rename(columns={'Network':'ResultFile'},inplace=True)\n",
    "        catchment_df_all = catchment_df_all[['ResultFile', 'MUID', 'DateTimeRef', 'Discharge', 'Volume']]\n",
    "\n",
    "    results = []\n",
    "    first_level = True\n",
    "    first_flow = True\n",
    "    first_velocity = True\n",
    "\n",
    "    for f in os.listdir(result_folder):\n",
    "        if f[-6:]=='.res1d' and not 'ADDOUT' in f and not 'RR' in f and not 'UserSpecified' in f and not 'hotstart' in f.lower():\n",
    "            res1d = Res1D(result_folder + '\\\\' + f)\n",
    "            reaches = res1d.data.Reaches\n",
    "            nodes = res1d.data.Nodes\n",
    "\n",
    "    #         print (\"Importing network \" + f + \" at \" + str(dt.datetime.now()))\n",
    "\n",
    "            for i, node in enumerate(nodes):\n",
    "\n",
    "                muid = node.Id\n",
    "                if muid in level_nodes:\n",
    "    #                 print (\"Importing node \" + node.Id + \" at \" + str(dt.datetime.now()))\n",
    "\n",
    "                    level_df = pd.DataFrame(index = res1d.time_index)\n",
    "                    level_df['ResultFile'] = f\n",
    "                    level_df['MUID'] = muid\n",
    "                    level_df['Level'] = res1d.query.GetNodeValues(muid, \"WaterLevel\")                          \n",
    "\n",
    "                    level_df['DateTimeRef'] = level_df.index\n",
    "\n",
    "                    if first_level == True:\n",
    "                        level_df_all = level_df.copy()\n",
    "                    else:\n",
    "                        level_df_all = pd.concat([level_df_all,level_df])\n",
    "                    first_level = False\n",
    "\n",
    "\n",
    "            first_round = True\n",
    "            for i, reach in enumerate(reaches):\n",
    "\n",
    "                muid = reach.Id[:reach.Id.rfind('-')]\n",
    "\n",
    "                if muid in us_level_pipes or muid in ds_level_pipes:\n",
    "\n",
    "                    if muid in us_level_pipes: \n",
    "                        values = res1d.query.GetReachStartValues(muid, \"WaterLevel\")\n",
    "                    else:\n",
    "                        values = res1d.query.GetReachEndValues(muid, \"WaterLevel\")\n",
    "\n",
    "                    level_df = pd.DataFrame(index = res1d.time_index)\n",
    "                    level_df['ResultFile'] = f\n",
    "                    level_df['MUID'] = muid\n",
    "                    level_df['Level'] = values                          \n",
    "\n",
    "                    level_df['DateTimeRef'] = level_df.index\n",
    "\n",
    "                    if first_level == True:\n",
    "                        level_df_all = level_df.copy()\n",
    "\n",
    "                    else:\n",
    "                        level_df_all = pd.concat([level_df_all,level_df])\n",
    "                    first_level = False\n",
    "\n",
    "\n",
    "\n",
    "            for i, reach in enumerate(reaches):\n",
    "\n",
    "                muid = reach.Id[:reach.Id.rfind('-')]\n",
    "\n",
    "                if muid in flow_pipes or muid in list(summation_df.MUID):\n",
    "\n",
    "                    values = res1d.query.GetReachEndValues(muid, \"Discharge\")\n",
    "                    flow_df = pd.DataFrame(index = res1d.time_index)\n",
    "                    flow_df['ResultFile'] = f\n",
    "                    flow_df['MUID'] = muid\n",
    "                    flow_df['DateTimeRef'] = flow_df.index\n",
    "                    flow_df['Discharge'] = values                          \n",
    "                    flow_df['Discharge'] = flow_df['Discharge'] * 1000\n",
    "\n",
    "                    flow_df['Seconds'] = flow_df.index.to_series().diff().astype('timedelta64[s]').fillna(method='bfill').astype('int64')\n",
    "                    flow_df['Volume'] = flow_df.Discharge * flow_df.Seconds / 1000\n",
    "                    flow_df.drop(columns=['Seconds'],inplace=True)\n",
    "\n",
    "                    if first_flow == True:\n",
    "                        flow_df_all = flow_df.copy()\n",
    "\n",
    "                    else:\n",
    "                        flow_df_all = pd.concat([flow_df_all,flow_df])\n",
    "                    first_flow = False\n",
    "\n",
    "            for i, reach in enumerate(reaches):\n",
    "\n",
    "                muid = reach.Id[:(-1 * len(str(i)) - 1)]\n",
    "\n",
    "                if muid in flow_pipes:\n",
    "                    print (\"Importing pipe \" + reach.Id + \" velocity at \" + str(dt.datetime.now()))\n",
    "\n",
    "                    values = res1d.query.GetReachEndValues(muid, \"FlowVelocity\")\n",
    "                    velocity_df = pd.DataFrame(index = res1d.time_index)\n",
    "                    velocity_df['ResultFile'] = f\n",
    "                    velocity_df['MUID'] = muid\n",
    "                    velocity_df['DateTimeRef'] = velocity_df.index\n",
    "                    velocity_df['Velocity'] = values                          \n",
    "\n",
    "\n",
    "                    if first_velocity == True:\n",
    "                        velocity_df_all = velocity_df.copy()\n",
    "                    else:\n",
    "                        velocity_df_all = pd.concat([velocity_df_all,velocity_df])\n",
    "                    first_velocity = False\n",
    "\n",
    "            first_round = False\n",
    "\n",
    "    if len(catchments) > 0:\n",
    "        catchment_df_all_filter  = catchment_df_all.copy()\n",
    "        times = list(flow_df_all.DateTimeRef.unique())\n",
    "        catchment_df_all_filter.set_index(catchment_df_all_filter.DateTimeRef,inplace=True)\n",
    "        catchment_df_all_filter =  catchment_df_all_filter[catchment_df_all_filter['DateTimeRef'].isin(times)]\n",
    "        flow_df_all = pd.concat([flow_df_all,catchment_df_all_filter])\n",
    "\n",
    "    negatives = list(summation_df.query(\"MUID.str.endswith('-Negative')\").MUID.unique())\n",
    "    original_negatives = []\n",
    "    for negative in negatives:\n",
    "        original_negatives.append(negative[:-9])\n",
    "    negatives_df = flow_df_all[flow_df_all.MUID.isin(original_negatives)].copy()\n",
    "    negatives_df.MUID = negatives_df.MUID + '-Negative'\n",
    "    negatives_df.Discharge = negatives_df.Discharge * -1\n",
    "    negatives_df.Volume = negatives_df.Volume * -1\n",
    "\n",
    "    flow_df_all = pd.concat([flow_df_all,negatives_df])\n",
    "\n",
    "    #Summation\n",
    "    df_result_sum = pd.merge(flow_df_all,summation_df,how='inner',on=['MUID'])\n",
    "    df_result_sum = df_result_sum.groupby(['ResultFile','SUMTO','DateTimeRef']).agg({'Discharge':'sum','Volume':'sum'})\n",
    "    df_result_sum.reset_index(inplace=True)\n",
    "    df_result_sum.set_index('DateTimeRef',drop=False,inplace=True)\n",
    "    df_result_sum.rename(columns = {'SUMTO':'MUID'},inplace=True)\n",
    "\n",
    "    flow_df_all = pd.concat([flow_df_all,df_result_sum])\n",
    "\n",
    "    not_founds = []\n",
    "    for muid in summation_df.MUID.unique():\n",
    "        if not muid in flow_df_all.MUID.unique():\n",
    "            not_founds.append(muid)\n",
    "\n",
    "    if len(not_founds) > 0:\n",
    "        error_message = 'The following elements in summation_df were not found: \\n' \n",
    "        error_message += ','.join(not_founds) + '.\\n'\n",
    "        raise ValueError(error_message)\n",
    "\n",
    "except Exception as e: \n",
    "    error_message = str(e)\n",
    "    MessageBox(None, b'An error happened in permanent cell 6\\n\\n' + error_message.encode('utf-8'), b\"Error\", 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac6def4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#PERMANENT CELL 7\n",
    "#Create HTML reports\n",
    "\n",
    "try:\n",
    "    \n",
    "    shutil.copy2('style.css', output_folder + '\\\\style.css')\n",
    "    shutil.copy2('script.js', output_folder + '\\\\script.js')\n",
    "    \n",
    "    error_list = []\n",
    "    vol_stat_list = []\n",
    "    \n",
    "    for gauge in gauges.index:\n",
    "        zone = str(gauges.loc[gauge,'Location'])\n",
    "        if gauge in list(periods.Meter) and (zone in zone_filter or len(zone_filter) == 0):\n",
    "            location_type = gauges.loc[gauge,'Location Type']\n",
    "\n",
    "            pipe = str(gauges.loc[gauge,'Pipe'])\n",
    "            node = str(gauges.loc[gauge,'Node1 (Or Pipe if pipe level)'])\n",
    "            rain_gauge = str(gauges.loc[gauge,'Rain Gauge'])\n",
    "            MonitoringType = str(gauges.loc[gauge,\"Sensor Output\"])\n",
    "            NodeID = str(gauges.loc[gauge, \"Node1 (Or Pipe if pipe level)\"])\n",
    "            MHName = str(gauges.loc[gauge,'Node1 AssetName']) \n",
    "            number_catchments = wwf_specs.loc[zone,'Number sanitary'] \\\n",
    "                + wwf_specs.loc[zone,'Number combined'] \\\n",
    "                + wwf_specs.loc[zone,'Number stormwater']\n",
    "\n",
    "            effluent_types = []\n",
    "            if wwf_specs.loc[zone,'Number sanitary'] > 0:\n",
    "                effluent_types.append('Sanitary')\n",
    "            if wwf_specs.loc[zone,'Number combined'] > 0:\n",
    "                effluent_types.append('Combined')\n",
    "            if wwf_specs.loc[zone,'Number stormwater'] > 0:\n",
    "                effluent_types.append('Storm')\n",
    "\n",
    "            for i, effluent_type in enumerate(effluent_types):\n",
    "                if i == 0:\n",
    "                    effluent_type_string = effluent_type\n",
    "                elif i == len(effluent_types)-1:\n",
    "                    effluent_type_string += ' and ' + effluent_type\n",
    "                else:\n",
    "                    effluent_type_string += ', ' + effluent_type          \n",
    "\n",
    "\n",
    "            local_population = 0\n",
    "            for ww_type in ww_types:\n",
    "                if ww_type in res_types:                \n",
    "                    local_population += dwf_specs.loc[zone,ww_type + '_Population']\n",
    "\n",
    "            total_area = round(wwf_specs.loc[zone,'Drainage area (ha)'],1)\n",
    "\n",
    "            PipeShape = str(gauges.loc[gauge,'Pipe Shape'])\n",
    "            try:\n",
    "                PipeDimension = str(round(gauges.loc[gauge, 'Pipe Dimension'],2))\n",
    "            except:\n",
    "                PipeDimension = gauges.loc[gauge, 'Pipe Dimension']\n",
    "                \n",
    "            tabs = ['Report']\n",
    "            for p_no, period_spec in enumerate(period_specs):\n",
    "                description = period_spec[:2] + ' Calibration'\n",
    "                start = periods[periods.Meter==gauge][period_spec + ' Start'].iloc[0]\n",
    "                end = periods[periods.Meter==gauge][period_spec + ' End'].iloc[0]\n",
    "                if not pd.isnull(start):\n",
    "                    if start.month == end.month:\n",
    "                        tabs.append(period_spec[:3] + ', ' + start.strftime(\"%b %d\") + ' - ' + end.strftime(\"%d %Y\"))\n",
    "                    else:\n",
    "                        if start.year == end.year:\n",
    "                            tabs.append(period_spec[:3] + ', ' + start.strftime(\"%b %d\") + ' - ' + end.strftime(\"%b %d %Y\"))                \n",
    "                        else:\n",
    "                            tabs.append(period_spec[:3] + ', ' + start.strftime(\"%b %d %Y\") + ' - ' + end.strftime(\"%b %d %Y\"))                \n",
    "\n",
    "\n",
    "            with open(output_folder + \"\\\\\" + \"Calibration_Report_\" + gauge + \".html\", 'w') as f:                \n",
    "                \n",
    "                f.write('<!DOCTYPE html>\\n')\n",
    "                f.write('<html>\\n')\n",
    "                f.write('<head>\\n')\n",
    "                f.write('<meta charset=\"utf-8\">\\n')\n",
    "                f.write('<script src=\"script.js\"></script>\\n')\n",
    "                f.write('<link rel=\"stylesheet\" href=\"style.css\">\\n')\n",
    "                f.write('</head>\\n')\n",
    "                f.write('<body>\\n\\n')\n",
    "                \n",
    "                f.write('<div class=\"tab\">\\n')\n",
    "                for tab in tabs:\n",
    "                    f.write('  <button class=\"tablinks\" onclick=\"openTab(event, ' + \"'\" + tab + \"'\"  + ')\">' + tab + '</button>\\n')\n",
    "                f.write('</div>\\n')\n",
    "                \n",
    "                f.write('<div id=\"' + tabs[0] + '\" class=\"tabcontent\">\\n') \n",
    "                \n",
    "                f.write('<h1 style=\"text-align:center\">Model Calibration Report for gauge ' \n",
    "                        + gauge + ' in Zone ' + zone + ', ' + model_area + '</h1>')\n",
    "            \n",
    "                f.write('<h2>1. Description of the Calibration Area</h2>')\n",
    "                f.write('<table class=\"first\">\\n')\n",
    "                f.write('<tr>\\n')\n",
    "                f.write('<td>Number of Catchments</td>\\n')\n",
    "                f.write('<td>'+ str(number_catchments) +'</td>\\n')\n",
    "                f.write('</tr>\\n')\n",
    "                f.write('<tr>\\n')\n",
    "                f.write('<td>Zone Area (Ha)</td>\\n')\n",
    "                f.write('<td>'+ str(total_area) +'</td>\\n')\n",
    "                f.write('</tr>\\n')\n",
    "                f.write('<tr>\\n')\n",
    "                f.write('<td>Local Residential Population</td>\\n')\n",
    "                f.write('<td>'+ str(int(local_population)) +'</td>\\n')\n",
    "                f.write('</tr>\\n')\n",
    "                f.write('<tr>\\n')\n",
    "                f.write('<td>Effluent Type</td>\\n')\n",
    "                f.write('<td>'+ effluent_type_string +'</td>\\n')\n",
    "                f.write('</tr>\\n')            \n",
    "                for ww_type in ww_types:\n",
    "                    if ww_type not in res_types:\n",
    "                        f.write('<tr>\\n')\n",
    "                        f.write('<td>' + ww_type + ' Area (Ha)</td>\\n')\n",
    "    #                     f.write('<td>'+ str(round(dwf_specs.loc[zone,ww_type + '_' + description],1)) +'</td>\\n')\n",
    "                        f.write('<td>'+ str(round(dwf_specs.loc[zone,ww_type + '_Area'],1)) +'</td>\\n')\n",
    "                        f.write('</tr>\\n')\n",
    "                f.write('</tr>\\n')\n",
    "                f.write('</table>')\n",
    "\n",
    "                f.write('<h2>2. Description of the Sensor and Network</h2>')            \n",
    "                f.write('<table class=\"first\">\\n')\n",
    "                f.write('<tr>\\n')\n",
    "                f.write('<td>'+ 'Monitoring Type' +'</td>\\n')\n",
    "                f.write('<td>'+ MonitoringType +'</td>\\n')\n",
    "                f.write('</tr>\\n')\n",
    "                f.write('<tr>\\n')\n",
    "                f.write('<td>'+ 'Node ID' +'</td>\\n')\n",
    "                f.write('<td>'+ node +'</td>\\n')\n",
    "                f.write('</tr>\\n')\n",
    "                f.write('<tr>\\n')\n",
    "                f.write('<td>'+ 'Pipe ID' +'</td>\\n')\n",
    "                f.write('<td>'+ pipe +'</td>\\n')\n",
    "                f.write('</tr>\\n')\n",
    "                f.write('<tr>\\n')\n",
    "                f.write('<td>'+ 'Pipe Shape' +'</td>\\n')\n",
    "                f.write('<td>'+ PipeShape +'</td>\\n')\n",
    "                f.write('</tr>\\n')\n",
    "                f.write('</tr>\\n')\n",
    "                f.write('<tr>\\n')\n",
    "                f.write('<td>'+ 'Pipe Dimension (m) or CRS ID ' +'</td>\\n')\n",
    "                f.write('<td>'+ PipeDimension +'</td>\\n')\n",
    "                f.write('</tr>\\n')\n",
    "                f.write('</table>')\n",
    "\n",
    "\n",
    "                f.write('<h2>3. Model Calibration Setup: </h2>')\n",
    "\n",
    "                f.write('<table class=\"first\">\\n')\n",
    "                f.write('  <tr>\\n')\n",
    "                f.write('    <th>Simulation</th>\\n')\n",
    "                f.write('    <th>Start Date</th>\\n')\n",
    "                f.write('    <th>End Date</th>\\n')\n",
    "                f.write('    <th>Duration</th>\\n')\n",
    "                f.write('  </tr>\\n')\n",
    "\n",
    "                for period_spec in period_specs:\n",
    "                    description = period_spec[:2] + ' Calibration'\n",
    "                    start = periods[periods.Meter==gauge][period_spec + ' Start'].iloc[0]\n",
    "                    end = periods[periods.Meter==gauge][period_spec + ' End'].iloc[0]\n",
    "                    if pd.isnull(start):\n",
    "                        break\n",
    "                    duration = str((end - start).days) + ' days'\n",
    "\n",
    "                    f.write('  <tr>\\n')\n",
    "                    f.write('    <td>' + description + '</th>\\n')\n",
    "                    f.write('    <td>' + start.strftime(\"%Y-%m-%d %H:%M\") + '</th>\\n')\n",
    "                    f.write('    <td>' + end.strftime(\"%Y-%m-%d %H:%M\") + '</th>\\n')\n",
    "                    f.write('    <td>' + duration + '</th>\\n')\n",
    "                    f.write('  </tr>\\n')                   \n",
    "                f.write('</table>\\n')\n",
    "\n",
    "                f.write('<h2>4. Zone Map</h2>\\n')\n",
    "                f.write('<img src=\"' + map_folder + '\\\\' + zone + '.jpg\" alt=\"' + gauge + '\">\\n')\n",
    "\n",
    "                f.write('<h2>6. Diurnal Patterns</h2>\\n')  \n",
    "                f.write('<p>' + str(list(report_text[report_text.Zone==zone]['Diurnal Text'])[0]) + '</p>\\n')\n",
    "                fig_diurnal = go.Figure()\n",
    "                diurnals_filter1 = diurnals[diurnals.Zone==zone]\n",
    "                profile_ids = list(diurnals_filter1.Profile.unique())\n",
    "                for profile_id in profile_ids:\n",
    "                    diurnals_filter2 = diurnals_filter1[diurnals_filter1.Profile==profile_id]\n",
    "                    fig_diurnal.add_trace(go.Scatter(x=diurnals_filter2.Sqn, \n",
    "                                                 y = diurnals_filter2.Multiplier, \n",
    "                                                 mode='lines',name=profile_id + ' (' + diurnals_filter2.iloc[0,2] + ')'))             \n",
    "                fig_diurnal.update_layout(\n",
    "                    autosize=False,\n",
    "                    width = 1362,\n",
    "                    height=250,\n",
    "                    margin=dict(\n",
    "                        l=50,\n",
    "                        r=50,\n",
    "                        b=25,\n",
    "                        t=25,\n",
    "                        pad=4\n",
    "                        ),\n",
    "    #                 yaxis_title=\"Discharge (L/s)\" \n",
    "                    )\n",
    "\n",
    "                f.write(fig_diurnal.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "\n",
    "                f.write('<h2>6. DWF Calibration Summary</h2>\\n')  \n",
    "                f.write('<p>' + str(list(report_text[report_text.Zone==zone]['DWF Text'])[0]) + '</p>\\n')\n",
    "                f.write('<h2>7. WWF Calibration and Validation Summary</h2>\\n')\n",
    "                f.write('<p>' + str(list(report_text[report_text.Zone==zone]['WWF Text'])[0]) + '</p>\\n')\n",
    "                f.write('<h2>8. Calibration Issues</h2>\\n')\n",
    "                f.write('<p>' + str(list(report_text[report_text.Zone==zone]['Issue Text'])[0]) + '</p>\\n')\n",
    "                f.write('<h2>9. Recommendations</h2>\\n')\n",
    "                f.write('<p>' + str(list(report_text[report_text.Zone==zone]['Recommendation Text'])[0]) + '</p>\\n')                \n",
    "                f.write('</div>\\n')\n",
    "\n",
    "                wwf_plot_header_added = False\n",
    "                for p_no, period_spec in enumerate(period_specs):\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    start = periods[periods.Meter==gauge][period_spec + ' Start'].iloc[0]\n",
    "                    end = periods[periods.Meter==gauge][period_spec + ' End'].iloc[0]\n",
    "\n",
    "                    if pd.isnull(start):\n",
    "                        break  \n",
    "                        \n",
    "                    f.write('<div id=\"' + tabs[p_no + 1] + '\" class=\"tabcontent\">\\n') \n",
    "\n",
    "                    measured_gauge = measured.loc[(measured.Gauge==gauge) & (measured.index >= start) & (measured.index <= end)].copy()\n",
    "                                        \n",
    "                    if location_type == 'PS':\n",
    "                        measured_gauge['Discharge_Hourly'] = measured_gauge['Flow'].rolling('1h').mean()\n",
    "\n",
    "                    measured_rain = rainfall.loc[(rainfall.index >= start) & (rainfall.index <= end)][rain_gauge].to_frame()\n",
    "\n",
    "                    result_flow_gauge = flow_df_all.loc[(flow_df_all.MUID==pipe) & (flow_df_all.index >= start) & (flow_df_all.index <= end)].copy()\n",
    "                    if len(result_flow_gauge.ResultFile.unique()) > 1:\n",
    "                        result_file_counts = result_flow_gauge['ResultFile'].value_counts()\n",
    "                        most_common_result_file = result_file_counts.idxmax()\n",
    "                        result_flow_gauge = result_flow_gauge[result_flow_gauge['ResultFile'] == most_common_result_file]\n",
    "\n",
    "                    result_velocity_gauge = velocity_df_all.loc[(velocity_df_all.MUID==pipe) & (velocity_df_all.index >= start) & (velocity_df_all.index <= end)].copy()\n",
    "                    if len(result_velocity_gauge.ResultFile.unique()) > 1:\n",
    "                        result_file_counts = result_velocity_gauge['ResultFile'].value_counts()\n",
    "                        most_common_result_file = result_file_counts.idxmax()\n",
    "                        result_velocity_gauge = result_velocity_gauge[result_velocity_gauge['ResultFile'] == most_common_result_file]\n",
    "                    \n",
    "                    spill_df_zone = spill_df_zones.loc[(spill_df_zones.Zone==zone) & (spill_df_zones.Date_Time >= start) & (spill_df_zones.Date_Time <= end)].copy()                \n",
    "                    spill_df_all_zone = spill_df_all.loc[(spill_df_all.Zone==zone) & (spill_df_all.index >= start) & (spill_df_all.index <= end)].copy()\n",
    "                    spill_df_all_zone.drop(columns=['Zone'],inplace=True)\n",
    "                    spill_top_ten = spill_df_all_zone.groupby(['Node']).max()\n",
    "                    spill_top_ten.reset_index(inplace=True)\n",
    "                    spill_top_ten.sort_values(by='Spill',ascending=False,inplace=True)\n",
    "                    spill_top_ten.reset_index(drop=True,inplace=True)\n",
    "                    spill_top_ten = spill_top_ten[spill_top_ten['Spill']>0]\n",
    "                    spill_top_ten = spill_top_ten.head(10)                                \n",
    "\n",
    "                    if location_type == 'PS':\n",
    "                        result_flow_gauge['Discharge_Hourly'] = result_flow_gauge['Discharge'].rolling('1h').mean()\n",
    "\n",
    "                    result_level_gauge = level_df_all.loc[(level_df_all.MUID==node) & (level_df_all.index >= start) & (level_df_all.index <= end)]\n",
    "                    if len(result_level_gauge.ResultFile.unique()) > 1:\n",
    "                        result_file_counts = result_level_gauge['ResultFile'].value_counts()\n",
    "                        most_common_result_file = result_file_counts.idxmax()\n",
    "                        result_level_gauge = result_level_gauge[result_level_gauge['ResultFile'] == most_common_result_file]                    \n",
    "                   \n",
    "                    has_outfalls = False\n",
    "                    if zone in list(outfalls.Zone):\n",
    "                        has_outfalls = True\n",
    "                        outfalls_filter = outfalls[outfalls.Zone==zone].copy()\n",
    "                        outfalls_filter.reset_index(inplace=True)\n",
    "\n",
    "                        for index, row in outfalls_filter.iterrows():\n",
    "                            overflow_pipe = row['Res_ID']\n",
    "                            result_outfalls = flow_df_all.loc[(flow_df_all.MUID==overflow_pipe) & (flow_df_all.index >= start) & (flow_df_all.index <= end)].copy()\n",
    "                            result_outfalls['Outfall'] = row['Outfall']\n",
    "                            if index == 0:\n",
    "                                result_outfalls_all = result_outfalls.copy()\n",
    "                            else:\n",
    "                                result_outfalls_all = pd.concat([result_outfalls_all,result_outfalls])\n",
    "\n",
    "                #     result_gauge = df_result.loc[(df_result.index >= start) & (df_result.index <= end)]\n",
    "                    #---------------------------------------------------------------------------------------------\n",
    "                    compare_stats = []\n",
    "                    peak_level_model = round(result_level_gauge.Level.max(),2)\n",
    "                    compare_stats.append(['Peak HGL Model',peak_level_model,'m'])\n",
    "                    peak_level_measured = round(measured_gauge.Level.max(),2)\n",
    "                    compare_stats.append(['Peak HGL Measured',peak_level_measured,'m'])\n",
    "                    compare_stats.append(['Peak HGL Difference',round(peak_level_model - peak_level_measured,2),'m'])\n",
    "    #                 volume_model = round(result_flow_gauge.Volume.sum(),0).item()\n",
    "                    volume_model = round(result_flow_gauge.Volume.sum(),0)\n",
    "                    compare_stats.append(['Volume Model',volume_model,'m3'])\n",
    "                    volume_measured = round(measured_gauge.Volume.sum(),0).item() #convert to native float to avoid inf\n",
    "                    compare_stats.append(['Volume Measured',volume_measured,'m3'])\n",
    "                    try:\n",
    "                        vol_dif_pc = (volume_model - volume_measured) / volume_measured * 100\n",
    "                        compare_stats.append(['Volume Difference',round(vol_dif_pc, 0),'%'])\n",
    "                        if period_spec[:3] != \"DWF\":\n",
    "                            vol_stat_list.append([gauge,vol_dif_pc,start,end])\n",
    "                    except:\n",
    "                        compare_stats.append(['Volume Difference','','%'])\n",
    "                    peak_flow_model = round(result_flow_gauge.Discharge.max(),1)\n",
    "                    compare_stats.append(['Peak Flow Model',peak_flow_model,'L/s'])\n",
    "                    peak_flow_measured = round(measured_gauge.Flow.max(),1)\n",
    "                    compare_stats.append(['Peak Flow Measured',peak_flow_measured,'L/s'])\n",
    "                    try:\n",
    "                        compare_stats.append(['Peak Flow Difference',round((peak_flow_model - peak_flow_measured) / peak_flow_measured * 100, 0),'%'])\n",
    "                    except:\n",
    "                        compare_stats.append(['Peak Flow Difference','','%'])\n",
    "\n",
    "\n",
    "                    f.write('<h3> Gauge ' \n",
    "                        + gauge + ' in Zone ' + zone + '</h3>\\n')  \n",
    "                    \n",
    "                    if period_spec[:3] == \"DWF\":\n",
    "\n",
    "                        f.write('<div class=\"row\">')\n",
    "\n",
    "                        f.write('<div class=\"column\">')    \n",
    "                        f.write('<table class=\"second\">\\n')\n",
    "                        f.write('  <tr>\\n')\n",
    "                        f.write('    <th>Description</th>\\n')\n",
    "                        f.write('    <th>Value</th>\\n')\n",
    "                        f.write('    <th>Unit</th>\\n')\n",
    "                        f.write('  </tr>\\n')                   \n",
    "\n",
    "                        #sim and measured stats\n",
    "                        for compare_stat in compare_stats:\n",
    "                            f.write('  <tr>\\n')\n",
    "                            f.write('    <td>' + compare_stat[0] + '</th>\\n')\n",
    "                            f.write('    <td>' + str(compare_stat[1]) + '</th>\\n')\n",
    "                            f.write('    <td>' + compare_stat[2] + '</th>\\n')\n",
    "                            f.write('  </tr>\\n')\n",
    "\n",
    "                        f.write('</table>\\n')\n",
    "                        f.write('</div>')                    \n",
    "\n",
    "                        #Loading rate\n",
    "                        f.write('<div class=\"column\">')    \n",
    "                        f.write('<table class=\"second\">\\n')\n",
    "                        f.write('  <tr>\\n')\n",
    "                        f.write('    <th>Load Type</th>\\n')\n",
    "                        f.write('    <th>Loading Rate</th>\\n')\n",
    "                        f.write('    <th>Unit</th>\\n')\n",
    "                        f.write('  </tr>\\n')\n",
    "\n",
    "                        for ww_type in ww_types:\n",
    "                            if ww_type in res_types:\n",
    "                                decimals = 3\n",
    "                            else: \n",
    "                                decimals = 1\n",
    "                            rate = round(dwf_specs.loc[zone,ww_type + '_Rate'],decimals)\n",
    "                            unit = all_types.loc[ww_type,'Unit2']\n",
    "\n",
    "                            f.write('  <tr>\\n')\n",
    "                            f.write('    <td>' + ww_type + '</th>\\n')\n",
    "                            f.write('    <td>' + str(rate) + '</th>\\n')\n",
    "                            f.write('    <td>' + unit + '</th>\\n')\n",
    "                            f.write('  </tr>\\n') \n",
    "                        f.write('<tr><td><br/></td><td><br/></td><td><br/></td></tr>\\n')\n",
    "                        f.write('<tr><td><br/></td><td><br/></td><td><br/></td></tr>\\n')\n",
    "                        f.write('<tr><td><br/></td><td><br/></td><td><br/></td></tr>\\n')\n",
    "                        f.write('</table>\\n')\n",
    "                        f.write('</div>')\n",
    "\n",
    "                        f.write('<div class=\"column\">')\n",
    "\n",
    "                        #Population and ICI areas\n",
    "                        f.write('<table class=\"second\">\\n')\n",
    "                        f.write('  <tr>\\n')\n",
    "                        f.write('    <th>Load Type</th>\\n')\n",
    "                        f.write('    <th>Local</th>\\n')\n",
    "                        if use_accumulation == True:\n",
    "                            f.write('    <th>Total</th>\\n')\n",
    "                        else:\n",
    "                            f.write('    <th></th>\\n')\n",
    "                        f.write('    <th>Unit</th>\\n')\n",
    "                        f.write('  </tr>\\n')\n",
    "\n",
    "                        for ww_type in ww_types:\n",
    "                            if ww_type in res_types:\n",
    "                                decimals = 0\n",
    "                            else: \n",
    "                                decimals = 1\n",
    "                            description = unit = all_types.loc[ww_type,'Description']\n",
    "                            local_val = round(dwf_specs.loc[zone,ww_type + '_' + description],decimals)\n",
    "                            if use_accumulation == True:\n",
    "                                total_val = round(dwf_specs.loc[zone,ww_type + '_' + description + '_Upstream'],decimals)\n",
    "                            unit = all_types.loc[ww_type,'Unit1']\n",
    "\n",
    "                            f.write('  <tr>\\n')\n",
    "                            f.write('    <td>' + ww_type + '</th>\\n')\n",
    "                            f.write('    <td>' + str(local_val) + '</th>\\n')\n",
    "                            if use_accumulation == True:\n",
    "                                f.write('    <td>' + str(total_val) + '</th>\\n')\n",
    "                            else:\n",
    "                                f.write('    <td></th>\\n')\n",
    "                            f.write('    <td>' + unit + '</th>\\n')\n",
    "                            f.write('  </tr>\\n') \n",
    "                        f.write('<tr><td><br/></td><td><br/></td><td><br/></td><td><br/></td></tr>\\n')\n",
    "                        f.write('<tr><td><br/></td><td><br/></td><td><br/></td><td><br/></td></tr>\\n')\n",
    "                        f.write('<tr><td><br/></td><td><br/></td><td><br/></td></tr>\\n')\n",
    "                        f.write('</table>\\n')\n",
    "                        f.write('</div>')\n",
    "\n",
    "                        f.write('<div class=\"column\">')\n",
    "                        #Waterload\n",
    "                        f.write('<table class=\"second\">\\n')\n",
    "                        f.write('  <tr>\\n')\n",
    "                        f.write('    <th>Load Type</th>\\n')\n",
    "                        f.write('    <th>Local</th>\\n')\n",
    "                        if use_accumulation == True:\n",
    "                            f.write('    <th>Total</th>\\n')\n",
    "                        else:\n",
    "                            f.write('    <th></th>\\n')\n",
    "                        f.write('    <th>Unit</th>\\n')\n",
    "                        f.write('  </tr>\\n')\n",
    "\n",
    "                        for load_type in all_types.index.values.tolist():\n",
    "\n",
    "                            local_val = round(dwf_specs.loc[zone,load_type + '_WaterLoad'],0)\n",
    "                            if use_accumulation == True:\n",
    "                                total_val = round(dwf_specs.loc[zone,load_type + '_WaterLoad_Upstream'],0)\n",
    "                            unit = 'm3/d'\n",
    "\n",
    "                            f.write('  <tr>\\n')\n",
    "                            f.write('    <td>' + load_type + '</th>\\n')\n",
    "                            f.write('    <td>' + str(local_val) + '</th>\\n')\n",
    "                            if use_accumulation == True:\n",
    "                                f.write('    <td>' + str(total_val) + '</th>\\n')\n",
    "                            else:\n",
    "                                f.write('    <td></th>\\n')\n",
    "                            f.write('    <td>' + unit + '</th>\\n')\n",
    "                            f.write('  </tr>\\n') \n",
    "\n",
    "                        f.write('<tr><td><br/></td><td><br/></td><td><br/></td></tr>\\n')\n",
    "\n",
    "                        f.write('</table>\\n')\n",
    "                        f.write('</div>')\n",
    "    #                     f.write('</div>')\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        total_rows = 10\n",
    "                        f.write('<div class=\"row\">')\n",
    "                        for i in range(0,4): #Cycle through the three table types.   \n",
    "                            f.write('<div class=\"column\">')    \n",
    "                            f.write('<table class=\"second\">\\n')\n",
    "                            f.write('  <tr>\\n')\n",
    "                            f.write('    <th>Description</th>\\n')\n",
    "                            f.write('    <th>Value</th>\\n')\n",
    "                            f.write('    <th>Unit</th>\\n')\n",
    "                            f.write('  </tr>\\n')\n",
    "\n",
    "                            current_row = 0\n",
    "                            if i == 0:\n",
    "                                for compare_stat in compare_stats:\n",
    "                                    f.write('  <tr>\\n')\n",
    "                                    f.write('    <td>' + compare_stat[0] + '</th>\\n')\n",
    "                                    f.write('    <td>' + str(compare_stat[1]) + '</th>\\n')\n",
    "                                    f.write('    <td>' + compare_stat[2] + '</th>\\n')\n",
    "                                    f.write('  </tr>\\n')\n",
    "                                    current_row += 1                \n",
    "                            else:\n",
    "                                for index, row in wwf_stats_specs[wwf_stats_specs.Table==i].iterrows():\n",
    "\n",
    "                                    decimals = row['Decimals']\n",
    "\n",
    "\n",
    "                                    f.write('  <tr>\\n')\n",
    "                                    f.write('    <td>' + row['Description'] + '</th>\\n')\n",
    "                                    if row['DF'] == 'wwf_specs':\n",
    "                                        if slope_source_unit_meter_per_meter == False and row['Description'] == 'Average slope':\n",
    "                                            f.write('    <td>' + str(round(wwf_specs.loc[zone,row['DF_Col']]/1000,decimals)) + '</th>\\n')\n",
    "                                        elif 'max (mm)' in row['DF_Col']:\n",
    "                                            f.write('    <td>' + str(int(wwf_specs.loc[zone,row['DF_Col']]*1000)) + '</th>\\n')\n",
    "                                        elif decimals > 0:\n",
    "                                            f.write('    <td>' + str(round(wwf_specs.loc[zone,row['DF_Col']],decimals)) + '</th>\\n')\n",
    "                                        else:\n",
    "                                            f.write('    <td>' + str(int(wwf_specs.loc[zone,row['DF_Col']])) + '</th>\\n')\n",
    "                                    else: \n",
    "                                        f.write('    <td>' + str(round(measured_rain[rain_gauge].sum(),decimals)) + '</th>\\n')\n",
    "                                    f.write('    <td>' + row['Unit'] + '</th>\\n')\n",
    "                                    f.write('  </tr>\\n')\n",
    "                                    current_row += 1\n",
    "\n",
    "                            remaining_rows = total_rows - current_row\n",
    "\n",
    "                            for remaining_row in range(remaining_rows):\n",
    "                                f.write('<tr><td><br/></td><td><br/></td><td><br/></td></tr>\\n')\n",
    "                            f.write('</table>\\n')\n",
    "                            f.write('</div>')\n",
    "\n",
    "                    #Spill top 10\n",
    "                    if period_spec[:3] == \"DWF\":\n",
    "                        total_rows = 9\n",
    "                    else:\n",
    "                        total_rows = 10\n",
    "\n",
    "                    f.write('<div class=\"row\">')\n",
    "\n",
    "                    f.write('<div class=\"column\">')    \n",
    "                    f.write('<table class=\"second\">\\n')\n",
    "                    f.write('  <tr>\\n')\n",
    "                    f.write('    <th>Node Spill Top 10</th>\\n')\n",
    "                    f.write('    <th>Peak Spill (L/s)</th>\\n')\n",
    "                    f.write('  </tr>\\n')\n",
    "\n",
    "                    current_row = 0\n",
    "\n",
    "                    for index, row in spill_top_ten.iterrows():\n",
    "                        f.write('  <tr>\\n')\n",
    "                        f.write('    <td>' + row['Node'] + '</th>\\n')\n",
    "                        f.write('    <td>' + str(int(row['Spill'])) + '</th>\\n')\n",
    "                        f.write('  </tr>\\n')\n",
    "                        current_row += 1                \n",
    "\n",
    "\n",
    "                    remaining_rows = total_rows - current_row\n",
    "\n",
    "                    for remaining_row in range(remaining_rows):\n",
    "                        f.write('<tr><td><br/></td><td><br/></td></tr>\\n')\n",
    "                    f.write('</table>\\n')\n",
    "                    f.write('</div>')\n",
    "\n",
    "\n",
    "                    f.write('</div>')\n",
    "                        #----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "                    fig = plotly.subplots.make_subplots(rows=1,cols=2,subplot_titles=(\n",
    "                                                        'Discharge at Gauge ' + gauge + ', Reach ' + pipe + ', Rain Gauge ' + rain_gauge,  \n",
    "                                                        'HGL at Gauge ' + gauge + ', Node ' + node),\n",
    "                                                        specs=[[{\"secondary_y\": True}, {\"secondary_y\": True}]],\n",
    "                                                       horizontal_spacing = 0.1)\n",
    "\n",
    "                    fig.add_trace(go.Scatter(x=result_flow_gauge.DateTimeRef, y=result_flow_gauge.Discharge, mode='lines',name='Model Flow'),1,1)   \n",
    "                    fig.add_trace(go.Scatter(x=measured_gauge.index, y=measured_gauge.Flow, mode='lines',name='Measured Flow'),1,1)\n",
    "                    fig.add_trace(go.Scatter(x=measured_rain.index, y=measured_rain[rain_gauge], mode='lines',name='Rainfall'),1,1, secondary_y=True)\n",
    "\n",
    "                    if location_type == 'PS':\n",
    "                        fig.add_trace(go.Scatter(x=result_flow_gauge.DateTimeRef, y=result_flow_gauge.Discharge_Hourly, mode='lines',name='Model Flow Hourly',line_color='yellow'),1,1)   \n",
    "                        fig.add_trace(go.Scatter(x=measured_gauge.index, y=measured_gauge.Discharge_Hourly, mode='lines',name='Measured Flow Hourly',line_color='black'),1,1)\n",
    "\n",
    "                    fig.add_trace(go.Scatter(x=result_level_gauge.index, y=result_level_gauge.Level, mode='lines',name='Model Level',line_color='green'),1,2)\n",
    "                    fig.add_trace(go.Scatter(x=measured_gauge.index, y=measured_gauge.Level, mode='lines',name='Measured Level',line_color='purple'),1,2)\n",
    "                    \n",
    "                    invert = network_specs.loc[node,'Invert']\n",
    "                    ground = network_specs.loc[node,'Ground']\n",
    "                    soh = network_specs.loc[node,'SOH']\n",
    "                    \n",
    "                    fig.add_trace(go.Scatter(x=[start,end], y=[invert,invert], mode='lines',name='Invert Level',line={'color': 'black','dash': 'dash'}),1,2)\n",
    "                    fig.add_trace(go.Scatter(x=[start,end], y=[ground,ground], mode='lines',name='Ground Level',line={'color': 'brown','dash': 'dash'}),1,2)\n",
    "                    if not math.isnan(soh):\n",
    "                        fig.add_trace(go.Scatter(x=[start,end], y=[soh,soh], mode='lines',name='SOH',line={'color': 'red','dash': 'dash'}),1,2)         \n",
    "                    \n",
    "                    chart_header = period_spec[:3] + ', ' + start.strftime(\"%b %d %Y\") + ' - ' + end.strftime(\"%b %d %Y\")\n",
    "                    \n",
    "                    fig.update_layout(\n",
    "                        autosize=False,\n",
    "                        width = 1600,\n",
    "                        height=280,\n",
    "                        margin=dict(\n",
    "                            l=0,\n",
    "                            r=0,\n",
    "                            b=25,\n",
    "                            t=60,\n",
    "                            pad=4\n",
    "                            ),\n",
    "\n",
    "                        title ={\n",
    "                            'text' : chart_header,\n",
    "                            'x':0.45,\n",
    "                            'xanchor': 'center'\n",
    "                        })\n",
    "\n",
    "                    fig['layout']['yaxis']['title']='Discharge (L/s)'\n",
    "                    fig['layout']['yaxis2']['title']='Rainfall (mm / 5 min)' \n",
    "                    fig['layout']['yaxis2']['range']=[10,0]\n",
    "                    fig['layout']['yaxis3']['title']='HGL (m)'\n",
    "\n",
    "                    f.write(fig.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "                    f.write('<br>')\n",
    "\n",
    "                    fig = plotly.subplots.make_subplots(rows=1,cols=2,subplot_titles=(\n",
    "                                                        'Velocity at Gauge ' + gauge + ', Reach ' + pipe,\n",
    "                                                        'Total Manhole Spills, Weir Overtopping, CSO and SSO'),\n",
    "                                                        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]],\n",
    "                                                       horizontal_spacing = 0.1)\n",
    "\n",
    "                    fig.add_trace(go.Scatter(x=result_velocity_gauge.DateTimeRef, y=result_velocity_gauge.Velocity, mode='lines',name='Model Velocity'),1,1)   \n",
    "                    fig.add_trace(go.Scatter(x=measured_gauge.index, y=measured_gauge.Velocity, mode='lines',name='Measured Velocity'),1,1)\n",
    "\n",
    "                    spill_df_zone\n",
    "                    fig.add_trace(go.Scatter(x=spill_df_zone.Date_Time, y=spill_df_zone.Spill, mode='lines',name='Total Manhole Spilling'),1,2)\n",
    "                    if has_outfalls == True:\n",
    "\n",
    "                        show_first = []\n",
    "                        show_last = []\n",
    "                        for outfall in list(outfalls_filter.Outfall):\n",
    "                            if outfall in measured.Gauge.unique():\n",
    "                                show_first.append(outfall)\n",
    "                            else:\n",
    "                                show_last.append(outfall)\n",
    "                        outfalls_to_plot = show_first + show_last\n",
    "                        for i, outfall in enumerate(outfalls_to_plot):\n",
    "                            result_outfalls_all_filter = result_outfalls_all[result_outfalls_all.Outfall==outfall]\n",
    "                            if i < len(colors):\n",
    "                                fig.add_trace(go.Scatter(x=result_outfalls_all_filter.index, \n",
    "                                                         y = result_outfalls_all_filter.Discharge, \n",
    "                                                         mode='lines',name=outfall,line={'color': colors[i]}),1,2)\n",
    "                            else:\n",
    "                                fig.add_trace(go.Scatter(x=result_outfalls_all_filter.index, \n",
    "                                                         y = result_outfalls_all_filter.Discharge, \n",
    "                                                         mode='lines',name=outfall),1,2)\n",
    "\n",
    "                            if outfall in measured.Gauge.unique():\n",
    "                                outfall_measured = measured.loc[(measured.Gauge==outfall) & (measured.index >= start) & (measured.index <= end)].copy()    \n",
    "                                if i < len(colors):\n",
    "                                    fig.add_trace(go.Scatter(x=outfall_measured.index, \n",
    "                                                             y=outfall_measured.Flow, \n",
    "                                                             mode='lines',name=outfall + ' - Measured',\n",
    "                                                             line={'color': colors[i],'dash': 'dash'}),1,2)                                                         \n",
    "                                else:\n",
    "                                    fig.add_trace(go.Scatter(x=outfall_measured.index, \n",
    "                                                             y=outfall_measured.Flow, \n",
    "                                                             mode='lines',name=outfall + ' - Measured',\n",
    "                                                             line={'dash': 'dash'}),1,2)\n",
    "\n",
    "\n",
    "                    fig.update_layout(\n",
    "                        autosize=False,\n",
    "                        width = 1600,\n",
    "                        height=280,\n",
    "                        margin=dict(\n",
    "                            l=0,\n",
    "                            r=0,\n",
    "                            b=25,\n",
    "                            t=60,\n",
    "                            pad=4\n",
    "                            ))\n",
    "\n",
    "\n",
    "                    fig['layout']['yaxis']['title']='Velocity (m/s)'\n",
    "                    fig['layout']['yaxis2']['title']='Discharge (L/s)'\n",
    "\n",
    "                    f.write(fig.to_html(full_html=False, include_plotlyjs='cdn'))\n",
    "                    f.write('<br>\\n')\n",
    "                    \n",
    "                    f.write('</div>\\n')\n",
    "                    f.write('</div>\\n')\n",
    "                    \n",
    "                f.write('</body>\\n')\n",
    "                f.write('</html>\\n')\n",
    "\n",
    "            f.close()\n",
    "            print(\"Writing html for \" + gauge)\n",
    "            \n",
    "    #Create csv file for suggested WWF dates for confidence maps based on smallest volume differences.\n",
    "    if generate_confidence_csvs:\n",
    "        vol_stat_list_df = pd.DataFrame(vol_stat_list,columns=['Gauge','Diff','Start','End'])\n",
    "        vol_stat_list_df = vol_stat_list_df[~np.isinf(vol_stat_list_df['Diff'])]\n",
    "        vol_stat_list_df.Diff = abs(vol_stat_list_df.Diff)\n",
    "        vol_stat_list_df.sort_values(['Gauge','Diff'],inplace=True)\n",
    "        vol_stat_list_df = vol_stat_list_df.groupby('Gauge').head(2).reset_index(drop=True)\n",
    "        vol_stat_list_df.reset_index(inplace=True,drop=True)\n",
    "        vol_stat_list_df = (vol_stat_list_df\n",
    "                      .set_index(['Gauge', vol_stat_list_df.groupby('Gauge').cumcount() + 1])\n",
    "                      .unstack()\n",
    "                      .sort_index(axis=1, level=1))  # Ensure columns are in the correct order\n",
    "        vol_stat_list_df.columns = [f\"{col}{num}\" for col, num in vol_stat_list_df.columns]\n",
    "        vol_stat_list_df = vol_stat_list_df.reset_index()\n",
    "        vol_stat_list_df = vol_stat_list_df[['Gauge', 'Diff1','Diff2','Start1','End1','Start2','End2']]\n",
    "        vol_stat_list_df.to_csv(output_folder + '\\\\Suggested_Confidence_Dates_Smallest_Volume_Differences.csv',index=False)\n",
    "\n",
    "except Exception as e: \n",
    "    traceback.print_exc()\n",
    "    MessageBox(None,b'An error happened in permanent cell 7', b'Error', 0)\n",
    "    raise ValueError(\"Error\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d907cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#PERMANENT CELL 8\n",
    "#Create csv files for confidence maps.\n",
    "\n",
    "try:\n",
    "    \n",
    "    if generate_confidence_csvs == True:\n",
    "        \n",
    "        # sql function\n",
    "        def sql_to_df(sql,model):\n",
    "            con = sqlite3.connect(model)\n",
    "            df = pd.read_sql(sql, con)\n",
    "            con.close()\n",
    "            return df\n",
    "\n",
    "        sql = \"SELECT muid, invertlevel FROM msm_Node WHERE active=1\"\n",
    "        node_df = sql_to_df(sql, model)\n",
    "        node_df.set_index(['muid'], inplace=True)\n",
    "\n",
    "        sql = \"SELECT muid, uplevel, dwlevel, fromnodeid, tonodeid, \" \n",
    "        sql += \"CASE typeno WHEN 1 THEN diameter WHEN 2 THEN height ELSE 0 END AS pipe_height \"\n",
    "        sql += \"FROM msm_Link WHERE active=1 \"\n",
    "        pipe_df = sql_to_df(sql, model)\n",
    "        pipe_df.set_index(['muid'], inplace=True)\n",
    "    \n",
    "\n",
    "        cols = ['Gauge', 'Zone', 'Pipe', 'Node', 'Pipe Shape', 'Node Bottom Level (m)', 'Pipe Bottom Level (m)', \n",
    "                'Pipe Top Level (m)', 'Diameter/Height (m)', 'Result File', 'Start', 'End', 'Used for', 'Meter Rank', \n",
    "                'Model Peak Hourly Average Flow (L/s)', 'Measured Peak Hourly Average Flow (L/s)', 'Peak Difference Flow (L/s)',\n",
    "                'Peak Difference Flow (%)', 'Model Peak HGL (m)', 'Measured Peak HGL (m)', 'Peak Difference HGL (m)',\n",
    "                'Model Pipe Filled', 'Measured Pipe Filled', 'Model Volume (m3)', 'Measured Volume (m3)',\n",
    "                'Volume Difference (m3)', 'Volume Difference (%)', 'Threshold Category','HGL Status', \n",
    "                'Flow Status', 'Volume Status', 'Weighted Status', 'HGL Status Value', 'Flow Status Value', \n",
    "                'Volume Status Value', 'Flow Volume Average Status Value', 'Weighted Status Value', 'WL QA', 'Flow QA']\n",
    "        dtypes = ['object', 'object', 'object', 'object', 'object', 'float64', 'float64', \n",
    "                  'float64', 'float64', 'object', 'datetime64[ns]', 'datetime64[ns]', 'object', 'object', \n",
    "                  'float64', 'float64', 'float64', \n",
    "                  'object', 'object', 'float64', 'float64', \n",
    "                  'float64', 'float64', 'object', 'float64', 'object',\n",
    "                  'object', 'object', 'object', 'float64', 'float64', \n",
    "                  'float64', 'float64', 'float64', 'object', 'object']\n",
    "        col_specs = list(zip(cols,dtypes))\n",
    "        data_types = {col[0]: col[1] for col in col_specs}\n",
    "        summary_df = pd.DataFrame(columns=cols).astype(data_types)\n",
    "      \n",
    "        color_dict = {}\n",
    "        color_dict[1] = 'Green'\n",
    "        color_dict[1.5] = 'Light Green'\n",
    "        color_dict[2] = 'Yellow'\n",
    "        color_dict[2.5] = 'Light Red'\n",
    "        color_dict[3] = 'Red'\n",
    "\n",
    "        event_categories = ['DWF Calibration','WWF Calibration','WWF Validation']\n",
    "\n",
    "        result_types = ['HGL','Flow','Volume']\n",
    "\n",
    "        result_points = []\n",
    "        zone_review = []\n",
    "\n",
    "        zone_dict = {}\n",
    "        for index, row in map_periods.iterrows():\n",
    "            gauge = row['Meter']\n",
    "            node = str(gauges.loc[gauge,'Node1 (Or Pipe if pipe level)'])\n",
    "            pipe = str(gauges.loc[gauge,'Pipe'])\n",
    "            zone = row['Zone']\n",
    "            zone_dict[gauge] = [zone,row['Meter Status']]\n",
    "            if zone in zone_filter or len(zone_filter) == 0:\n",
    "                meter_status = row['Meter Status']\n",
    "\n",
    "                x = gauges.loc[gauge,'X coordinate']\n",
    "                y = gauges.loc[gauge,'Y coordinate']\n",
    "                x_shift = gauges.loc[gauge,'Shift X (m)']\n",
    "                y_shift = gauges.loc[gauge,'Shift Y (m)']\n",
    "                catchment_type = gauges.loc[gauge,'Catchment Type']\n",
    "\n",
    "                print('Processing ' + gauge + ' for confidence maps.')\n",
    "\n",
    "                for i, event_category in enumerate(event_categories):\n",
    "                    event_category_short = event_category[:7]\n",
    "                    start = row[event_category_short + ' Start']\n",
    "                    end = row[event_category_short + ' End']\n",
    "\n",
    "                    #Find the associated result file\n",
    "                    result_flow_gauge = flow_df_all.loc[(flow_df_all.MUID==pipe) & (flow_df_all.index >= start) & (flow_df_all.index <= end)].copy()\n",
    "                    if len(result_flow_gauge.ResultFile.unique()) > 1:\n",
    "                        result_file_counts = result_flow_gauge['ResultFile'].value_counts()\n",
    "                        result_file = result_file_counts.idxmax()\n",
    "\n",
    "                    else:\n",
    "                        result_file = result_flow_gauge.ResultFile.unique()\n",
    "\n",
    "                    row_index = gauge + ' ' + event_category\n",
    "                    new_row_df = pd.DataFrame({'Gauge': gauge, \n",
    "                                               'Used for':event_category,\n",
    "                                               'Zone': zone,\n",
    "                                               'Start':start,\n",
    "                                               'End':end}, \n",
    "                                              index=[row_index])\n",
    "                    summary_df = pd.concat([summary_df, new_row_df])\n",
    "                    summary_df.loc[row_index, 'Meter Rank']=meter_status #ADDED 12/28/2023\n",
    "\n",
    "                    #This is gonna fail if level from pipe, must update\n",
    "                    summary_df.loc[row_index, 'Node Bottom Level (m)'] = node_df.loc[node, 'invertlevel'] #ADDED 12/29/2023\n",
    "                    if pd.isna(gauges.loc[gauge,'Pipe']) or pipe not in pipe_df.index:\n",
    "                        summary_df.loc[row_index, 'Diameter/Height (m)'] = np.nan\n",
    "                        summary_df.loc[row_index, 'Pipe Shape'] = np.nan\n",
    "                    else:\n",
    "                        summary_df.loc[row_index, 'Diameter/Height (m)'] = pipe_df.loc[pipe, 'pipe_height'] #ADDED 12/29/2023\n",
    "                        summary_df.loc[row_index, 'Pipe Shape'] = gauges.loc[gauge,'Pipe Shape'] #ADDED 12/29/2023\n",
    "                        #This is gonna fail if level from pipe, must update\n",
    "                        endnames = [['uplevel','fromnodeid'],['dwlevel','tonodeid']]\n",
    "                        pipe_level_found = False\n",
    "                        for endname in endnames:\n",
    "                            if node == pipe_df.loc[pipe, endname[1]]:\n",
    "                                if pd.isna(pipe_df.loc[pipe, endname[0]]):\n",
    "                                    pipe_level = node_df.loc[node, 'invertlevel']\n",
    "                                else:\n",
    "                                    pipe_level = pipe_df.loc[pipe, endname[0]]\n",
    "                                pipe_level_found = True\n",
    "                                summary_df.loc[row_index, 'Pipe Bottom Level (m)'] = pipe_level\n",
    "                        if pipe_level_found:\n",
    "                            pipe_top = pipe_level + summary_df.loc[row_index, 'Diameter/Height (m)']\n",
    "                            summary_df.loc[row_index, 'Pipe Top Level (m)'] = pipe_top                            \n",
    "                        else:\n",
    "                            print('Pipe level could not be determined.')                            \n",
    "                      \n",
    "                    summary_df.loc[row_index, 'Result File'] = result_file #ADDED 12/29/2023\n",
    "\n",
    "                    if type(start)==pd._libs.tslibs.timestamps.Timestamp:\n",
    "                        y_minor_shift = -i * map_point_spacing\n",
    "                        y_final = y + y_shift + y_minor_shift\n",
    "\n",
    "                        color_values = []\n",
    "                        for j, result_type in enumerate(result_types):\n",
    "                            x_minor_shift = j * map_point_spacing\n",
    "                            x_final = x + x_shift + x_minor_shift\n",
    "                            threshold_column = 'Threshold ' + result_type\n",
    "                            bad_data = False\n",
    "                            if result_type == 'HGL':\n",
    "                                result_type_csv = 'Max HGL'\n",
    "                                qa = row[event_category_short + ' WL QA']     \n",
    "                                summary_df.loc[row_index, 'WL QA']=qa #ADDED 12/29/2023\n",
    "                                if qa.lower() == 'bad data':\n",
    "                                    bad_data = True\n",
    "                                else:\n",
    "\n",
    "                                    summary_df.loc[row_index, 'Node']=node #reuse this line whenever find something that belongs in spreadsheet\n",
    "                                    max_model = level_df_all.loc[(level_df_all.MUID==node) & (level_df_all.index >= start) & (level_df_all.index <= end)].Level.max()\n",
    "                                    summary_df.loc[row_index, 'Model Peak HGL (m)']=max_model #ADDED 12/28/2023\n",
    "                                    max_measured = measured.loc[(measured.Gauge==gauge) & (measured.index >= start) & (measured.index <= end)].Level.max()\n",
    "                                    summary_df.loc[row_index, 'Measured Peak HGL (m)']=max_measured #ADDED 12/28/2023\n",
    "                                    if pipe_level_found:\n",
    "                                        summary_df.loc[row_index, 'Model Pipe Filled'] = max_model >= pipe_top\n",
    "                                    \n",
    "                                    if not math.isnan(max_measured) and max_measured != 0:\n",
    "                                        diff = abs(max_model - max_measured) * 100 #cm\n",
    "                                        summary_df.loc[row_index, 'Peak Difference HGL (m)']=diff/100 #ADDED 12/28/2023\n",
    "                                        if pipe_level_found:\n",
    "                                            summary_df.loc[row_index, 'Measured Pipe Filled'] = max_measured >= pipe_top\n",
    "                                        \n",
    "                                    else:\n",
    "                                        diff = 'NA'\n",
    "\n",
    "                            elif result_type == 'Flow':\n",
    "                                result_type_csv = 'Max Flow'\n",
    "                                qa = row[event_category_short + ' Flow QA']\n",
    "                                summary_df.loc[row_index, 'Flow QA']=qa #ADDED 12/29/2023\n",
    "                                if qa.lower() == 'bad data':\n",
    "                                    bad_data = True\n",
    "                                else:\n",
    "                                    pipe = str(gauges.loc[gauge,'Pipe'])\n",
    "                                    filter_model = flow_df_all.loc[(flow_df_all.MUID==pipe) & (flow_df_all.index >= start) & (flow_df_all.index <= end)].copy()\n",
    "                                    ts_seconds = (filter_model.index.max() - filter_model.index.min()).total_seconds() / (len(filter_model.index)-1)\n",
    "                                    ts_per_one_hour = int(60/ts_seconds)\n",
    "                                    #Remove overlaps\n",
    "                                    if len(filter_model.ResultFile.unique()) > 1:\n",
    "                                        row_counts = filter_model['ResultFile'].value_counts()\n",
    "                                        max_resultfile = row_counts.idxmax()\n",
    "                                        filter_model = filter_model[filter_model['ResultFile'] == max_resultfile]\n",
    "                                    if len(filter_model) == 0:\n",
    "                                        print('WARNING! ' + gauge + ' has no model records between ' + str(start) + ' and ' + str(end))                   \n",
    "                                    filter_model['Hourly'] = filter_model.Discharge.rolling('1h').mean()\n",
    "                                    filter_model = filter_model.iloc[ts_per_one_hour: , :] #Remove first hour to not get skewed hourly average there                        \n",
    "                                    max_model = filter_model.Hourly.max()\n",
    "                                    summary_df.loc[row_index, 'Model Peak Hourly Average Flow (L/s)']=max_model #ADDED 12/28/2023\n",
    "\n",
    "                                    filter_measured = measured.loc[(measured.Gauge==gauge) & (measured.index >= start) & (measured.index <= end)].copy()\n",
    "                                    try:\n",
    "                                        filter_measured['Hourly'] = filter_measured.Flow.rolling('1h').mean()\n",
    "                                    except:\n",
    "                                        filter_measured['Hourly'] = np.nan                           \n",
    "                                    \n",
    "                                    filter_measured = filter_measured.iloc[ts_per_one_hour: , :] #Remove first hour to not get skewed hourly average there \n",
    "                                    max_measured = filter_measured.Hourly.max()\n",
    "                                    print(max_measured)\n",
    "                                    summary_df.loc[row_index, 'Measured Peak Hourly Average Flow (L/s)']=max_measured #ADDED 12/28/2023\n",
    "                                    if not math.isnan(max_measured) and max_measured != 0:\n",
    "                                        diff_q = max_model - max_measured\n",
    "                                        diff = abs(diff_q) / max_measured\n",
    "                                        summary_df.loc[row_index, 'Peak Difference Flow (L/s)']=diff_q\n",
    "                                        summary_df.loc[row_index, 'Peak Difference Flow (%)']=diff\n",
    "                                    else:\n",
    "                                        diff = 'NA'\n",
    "                            else:\n",
    "                                result_type_csv = 'Acc Volume'\n",
    "                                qa = row[event_category_short + ' Flow QA']\n",
    "                                if qa.lower() == 'bad data':\n",
    "                                    bad_data = True\n",
    "                                else:\n",
    "\n",
    "                                    summary_df.loc[row_index, 'Pipe']=pipe #ADDED 12/28/2023\n",
    "                                    vol_model = flow_df_all.loc[(flow_df_all.MUID==pipe) & (flow_df_all.index >= start) & (flow_df_all.index <= end)].Volume.sum()\n",
    "                                    summary_df.loc[row_index, 'Model Volume (m3)']=vol_model #ADDED 12/28/2023\n",
    "                                    \n",
    "                                    if not math.isnan(max_measured) and max_measured != 0:\n",
    "                                        vol_measured = measured.loc[(measured.Gauge==gauge) & (measured.index >= start) & (measured.index <= end)].Volume.sum()\n",
    "                                        summary_df.loc[row_index, 'Measured Volume (m3)']=vol_measured #ADDED 12/28/2023\n",
    "                                        diff_vol = abs(vol_model - vol_measured)\n",
    "                                        diff = diff_vol/vol_measured\n",
    "                                        summary_df.loc[row_index, 'Volume Difference (m3)']=diff_vol #ADDED 12/28/2023\n",
    "                                        summary_df.loc[row_index, 'Volume Difference (%)']=diff #ADDED 12/28/2023\n",
    "\n",
    "                            status_column = result_type + ' Status'\n",
    "                            status_value_column = status_column + ' Value'\n",
    "                            if bad_data == True:\n",
    "                                color = 'Black'\n",
    "                                color_value = 0\n",
    "                            elif diff == 'NA':\n",
    "                                color = 'Grey'\n",
    "                                color_value = 0\n",
    "                            else:\n",
    "                                if event_category[:3] == 'WWF':\n",
    "                                    threshold_category = event_category[:3] + ' ' + catchment_type\n",
    "                                    status_lookup_column = threshold_category + ' ' + result_type + ' Status'\n",
    "                                else:\n",
    "                                    threshold_category = 'DWF'\n",
    "                                    status_lookup_column = event_category[:3] + ' ' + result_type + ' Status'\n",
    "                                \n",
    "                                for k in range(3):\n",
    "                                    threshold = thresholds.loc[k,status_lookup_column]\n",
    "                                    if diff >= threshold:\n",
    "                                        color = thresholds.loc[k,'Color']\n",
    "                                        #summary_df.loc[row_index, 'HGL Status']=color #ADDED 12/28/2023 ?????\n",
    "                                        color_value = thresholds.loc[k,'index']                                    \n",
    "                                        break\n",
    "                                        \n",
    "                                print(f'{status_lookup_column}, {threshold_category}, Diff: {diff}, {color}')\n",
    "                                    \n",
    "                            summary_df.loc[row_index, 'Threshold Category']=threshold_category #ADDED 12/28/2023\n",
    "                            summary_df.loc[row_index, status_column] = color\n",
    "                            summary_df.loc[row_index, status_value_column] = color_value\n",
    "                            \n",
    "                                \n",
    "\n",
    "                            color_values.append(color_value)\n",
    "\n",
    "                            label = ''\n",
    "                            if i == 0 and j == 0:\n",
    "                                label = gauge\n",
    "\n",
    "                            result_points.append([gauge,x_final,y_final,label,event_category,result_type_csv,color])\n",
    "                            summary_df.loc[row_index,result_type + ' Status'] = color\n",
    "                        summary_df.loc[row_index,'Flow Volume Average Status Value'] = (summary_df.loc[row_index,'Flow Status Value'] + summary_df.loc[row_index,'Volume Status Value']) / 2\n",
    "                        summary_df.loc[row_index,'Weighted Status Value'] = (summary_df.loc[row_index,'HGL Status Value'] + summary_df.loc[row_index,'Flow Volume Average Status Value']) / 2\n",
    "                        \n",
    "                        \n",
    "                        if meter_status == 'Primary':\n",
    "                            weighting_components = []\n",
    "                            weighted_color_value = 0\n",
    "                            value_count = 0\n",
    "                            qa = 'OK'\n",
    "                            for k, color_value in enumerate(color_values):\n",
    "                                if color_value > 0:\n",
    "                                    if k == 0:\n",
    "                                        weighting_components.append(color_value) #Level is weighted as 50%\n",
    "                                        value_count += 1\n",
    "                                    else:\n",
    "                                        weighting_components.append(color_value/2) #Flow is weighted as 25%\n",
    "                                        value_count += 0.5\n",
    "                                else:\n",
    "                                    if k == 0:\n",
    "                                        qa = 'Level Issue'\n",
    "                                    else:\n",
    "                                        qa = 'Flow Issue'                       \n",
    "\n",
    "                            if len(weighting_components) == 0:\n",
    "                                weighted_color_value = 0\n",
    "                                weighted_color = ''\n",
    "                                qa = 'All Data Issue'\n",
    "                            else:\n",
    "                                sum_color_values = 0\n",
    "                                for weighting_component in weighting_components:\n",
    "                                    sum_color_values += weighting_component\n",
    "                                weighted_color_value = sum_color_values / value_count\n",
    "                                weighted_color_value = round((weighted_color_value+0.001)*2,0)/2 #2.25 should round up to 2.5 as in Excel\n",
    "                                weighted_color = color_dict[weighted_color_value]\n",
    "                                summary_df.loc[row_index, 'Weighted Status']=weighted_color #ADDED 12/28/2023 \n",
    "                            zone_key = zone + ' ' + event_category + ' Weighted'\n",
    "                            zone_review.append([zone_key,'Weighted',event_category,zone,weighted_color,qa])\n",
    "\n",
    "        \n",
    "        result_points = pd.DataFrame(result_points,columns=['Gauge','X','Y','Label','SimType','ResultType','Color'])\n",
    "        for index, row in result_points.iterrows():\n",
    "            zone = zone_dict[row['Gauge']][0]\n",
    "            meter_status = zone_dict[row['Gauge']][1]\n",
    "            zone_key = zone + ' ' + row['SimType'] + ' ' + row['ResultType']\n",
    "            if row['Color'] in ['Black','Grey']:\n",
    "                qa = 'Level Issue' if row['SimType'] == 'Max HGL' else 'Flow Issue'\n",
    "            else:\n",
    "                qa = 'OK'\n",
    "            if meter_status == 'Primary':\n",
    "                zone_review.append([zone_key,row['ResultType'],row['SimType'],zone,row['Color'],qa])\n",
    "            \n",
    "        result_points.to_csv(output_folder + '\\\\ResultPoints.csv',index=False)\n",
    "        summary_df.to_excel(output_folder + '\\\\Calibration_Review.xlsx')\n",
    "        zone_review = pd.DataFrame(zone_review,columns=['Zone_Key','Stat','Event','Location','Status','QA'])\n",
    "        zone_review.to_csv(output_folder + '\\\\Zone_Review.csv',index=False)\n",
    "\n",
    "except Exception as e: \n",
    "    error_message = str(e)\n",
    "    MessageBox(None, b'An error happened in permanent cell 8\\n\\n' + error_message.encode('utf-8'), b\"Error\", 0)\n",
    "    raise ValueError(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5226b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PERMANENT CELL 9\n",
    "MessageBox(None,b'All cells ran successfully.', b'Done', 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_mike",
   "language": "python",
   "name": "py39_mike"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
